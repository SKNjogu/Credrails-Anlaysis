# -*- coding: utf-8 -*-
"""Credrails assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wcg6nFsMBxRnff-tPU7xDszqicefY7AZ
"""

# Comprehensive analysis of transaction data in one code block
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import LabelEncoder
import re

# Load the data
transactions_df = pd.read_csv('sample_transactions_DS (1).csv')

# Display basic information
print("Dataset Overview:")
print(f"Number of transactions: {transactions_df.shape[0]}")
print(f"Number of features: {transactions_df.shape[1]}")
print("\
Sample data:")
print(transactions_df.head())

# Data preprocessing
# Convert Transaction_Date to datetime
transactions_df['Transaction_Date'] = pd.to_datetime(transactions_df['Transaction_Date'], format='%d/%m/%Y', errors='coerce')

# Convert Transaction_Time to proper time format
transactions_df['Transaction_Time'] = pd.to_datetime(transactions_df['Transaction_Time'], format='%H:%M:%S', errors='coerce').dt.time

# Exploratory Data Analysis

# 1. Transaction trend over time
daily_transactions = transactions_df.groupby('Transaction_Date').size().reset_index(name='Count')
plt.figure(figsize=(12,6))
plt.plot(daily_transactions['Transaction_Date'], daily_transactions['Count'], marker='o')
plt.title('Number of Transactions per Day')
plt.xlabel('Date')
plt.ylabel('Number of Transactions')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('transactions_over_time.png')
plt.show()

# 2. Average transaction amount by Classification_Tag
avg_amount_by_tag = transactions_df.groupby('Classification_Tag')['Amount'].mean().reset_index()
plt.figure(figsize=(10,6))
sns.barplot(data=avg_amount_by_tag, x='Classification_Tag', y='Amount')
plt.title('Average Transaction Amount by Classification Tag')
plt.xlabel('Classification Tag')
plt.ylabel('Average Amount')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('avg_amount_by_tag.png')
plt.show()

# 3. Distribution of transaction amounts
plt.figure(figsize=(10,6))
sns.histplot(transactions_df['Amount'], bins=20, kde=True)
plt.title('Distribution of Transaction Amounts')
plt.xlabel('Transaction Amount')
plt.ylabel('Frequency')
plt.tight_layout()
plt.savefig('transaction_amount_distribution.png')
plt.show()

# 4. Transaction status distribution
status_counts = transactions_df['Status'].value_counts()
plt.figure(figsize=(8,6))
plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Transaction Status Distribution')
plt.axis('equal')
plt.tight_layout()
plt.savefig('status_distribution.png')
plt.show()

# 5. Transaction time distribution (hour of day)
transactions_df['Hour'] = pd.to_datetime(transactions_df['Transaction_Time'], format='%H:%M:%S').dt.hour
hourly_transactions = transactions_df.groupby('Hour').size().reset_index(name='Count')
plt.figure(figsize=(12,6))
sns.barplot(data=hourly_transactions, x='Hour', y='Count')
plt.title('Transactions by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Number of Transactions')
plt.xticks(range(0, 24))
plt.tight_layout()
plt.savefig('transactions_by_hour.png')
plt.show()

# Machine Learning Models for Classification Tag Prediction

# Prepare the data
# Extract features from Description
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(transactions_df['Description'])
feature_names = vectorizer.get_feature_names_out()

# Encode the target variable
le = LabelEncoder()
y = le.fit_transform(transactions_df['Classification_Tag'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train and evaluate models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Support Vector Machine': SVC()
}

results = {}

for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    results[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1 Score': f1_score(y_test, y_pred, average='weighted')
    }

    # Print results
    print(f"--- {name} Performance ---")
    print(f"Accuracy: {results[name]['Accuracy']:.4f}")
    print(f"Precision: {results[name]['Precision']:.4f}")
    print(f"Recall: {results[name]['Recall']:.4f}")
    print(f"F1 Score: {results[name]['F1 Score']:.4f}")
    print("\
")

# Compare models
results_df = pd.DataFrame(results).T
plt.figure(figsize=(12, 6))
results_df.plot(kind='bar', figsize=(12, 6))
plt.title('Model Performance Comparison')
plt.xlabel('Model')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.tight_layout()
plt.savefig('model_comparison.png')
plt.show()

# Feature importance for Random Forest
rf_model = models['Random Forest']
if hasattr(rf_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': rf_model.feature_importances_
    })

    # Get top 10 features
    top_features = feature_importance.sort_values('importance', ascending=False).head(10)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=top_features, x='importance', y='feature')
    plt.title('Top 10 Important Features for Random Forest')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.savefig('feature_importance.png')
    plt.show()

    # Print top features
    print("Top 10 important features for Random Forest:")
    for feature, importance in zip(top_features['feature'], top_features['importance']):
        print(f"{feature}: {importance:.4f}")

print("Analysis complete!")

import pandas as pd

data = {
    'Model': ['Logistic Regression', 'Random Forest', 'Support Vector Machine'],
    'Accuracy': [0.9984, 0.9984, 0.9984],
    'Precision': [0.9968, 0.9968, 0.9968],
    'Recall': [0.9984, 0.9984, 0.9984],
    'F1 Score': [0.9975, 0.9975, 0.9975]
}

performance_df = pd.DataFrame(data)

print(performance_df.head())
print('done')

"""##### The analysis shows that transaction descriptions containing words like "card", "transfers", and "payments" are the most important features for classification.

##### Summary:

##### The Random Forest and SVM models show slightly higher overall performance when compared with the Logistic Regression model.

##### The Random Forest additionally provides insight into the most influential words contributing to the classification.

#### Insights
##### The model performs extremely well at classifying transactions based on descriptions
##### There are some inconsistencies in classification naming (e.g., "Card Payments", "CardPayments", "Card_Payments")
##### Most transactions are completed successfully, with refunds and chargebacks being relatively rare
"""